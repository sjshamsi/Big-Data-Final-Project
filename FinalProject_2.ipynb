{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed21826-4412-41bd-b444-f21bb5d36abe",
   "metadata": {},
   "source": [
    "Download from SDSSDR18 all spectra from the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bdf86d1-c7da-4852-b405-5c9a9519363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astroquery.sdss import SDSS\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b258d7dc-a11f-405a-9bea-0e85da828fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOURCE_ID</th>\n",
       "      <th>original_ext_source_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3647851003078664832</td>\n",
       "      <td>1237648703516442877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3661958424457651712</td>\n",
       "      <td>1237648704049840359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662377883848390912</td>\n",
       "      <td>1237648704586973290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4419851136648786304</td>\n",
       "      <td>1237648705670152326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4407884120813978752</td>\n",
       "      <td>1237648705678213667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133828</th>\n",
       "      <td>1779912581508627072</td>\n",
       "      <td>1237680273108960003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133829</th>\n",
       "      <td>309194386402373120</td>\n",
       "      <td>1237680285996155390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133830</th>\n",
       "      <td>1883704073988238336</td>\n",
       "      <td>1237680303715582376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133831</th>\n",
       "      <td>2866659811294043904</td>\n",
       "      <td>1237680308016841309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133832</th>\n",
       "      <td>1799404173890542080</td>\n",
       "      <td>1237680327333381385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133833 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SOURCE_ID  original_ext_source_id\n",
       "0       3647851003078664832     1237648703516442877\n",
       "1       3661958424457651712     1237648704049840359\n",
       "2       3662377883848390912     1237648704586973290\n",
       "3       4419851136648786304     1237648705670152326\n",
       "4       4407884120813978752     1237648705678213667\n",
       "...                     ...                     ...\n",
       "133828  1779912581508627072     1237680273108960003\n",
       "133829   309194386402373120     1237680285996155390\n",
       "133830  1883704073988238336     1237680303715582376\n",
       "133831  2866659811294043904     1237680308016841309\n",
       "133832  1799404173890542080     1237680327333381385\n",
       "\n",
       "[133833 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossmatch = pd.read_csv('/Users/tharacaba/Downloads/ProjectBig/gaia_results_all_batches.csv')\n",
    "crossmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6be6b7c9-f297-4751-b16f-10afab29b6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    plate    mjd  fiberid            bestObjID\n",
      "0     303  51615      205  1237648703516442877\n",
      "1     299  51671      179  1237648704049840359\n",
      "2     300  51943      309  1237648704586973290\n",
      "3     311  51665      443  1237648705670152326\n",
      "4     364  52000      380  1237648705678213667\n",
      "5     267  51608      300  1237648720142663801\n",
      "6     474  52000      142  1237648720677044549\n",
      "7     290  51941       21  1237648720698277967\n",
      "8     271  51883      200  1237648721219551336\n",
      "9     279  51984      396  1237648721225711706\n",
      "10    292  51609      589  1237648721773396235\n",
      "11    305  51613      609  1237648721784078676\n",
      "12    308  51662      546  1237648721786175579\n",
      "13    312  51689      471  1237648721789124738\n",
      "14    503  51999       85  1237648722830557410\n",
      "15    285  51930      579  1237648722841501766\n",
      "16    419  51879      492  1237649920574750877\n",
      "17    463  51908      534  1237649963532353548\n",
      "18    461  51910      361  1237649964066930743\n",
      "19    330  52370      257  1237650369407352875\n",
      "20    331  52368      121  1237650760243216505\n",
      "21    337  51997      221  1237650761322332240\n",
      "22   4053  55591       38  1237650761862086708\n",
      "23    266  51630      227  1237650796219793525\n",
      "24  12533  58931      684  1237650797288423578\n",
      "25    773  52376      353  1237651067347861536\n",
      "26    602  52072      530  1237651067892334780\n",
      "27    451  51908      145  1237651190287368342\n",
      "28    447  51877      554  1237651190821617882\n",
      "29    439  51877      288  1237651191353507940\n",
      "30   6391  56329      988  1237651191354687536\n",
      "31    453  51915      372  1237651192436687028\n",
      "32    351  51695      212  1237651211749359981\n",
      "33    351  51695      493  1237651212285771850\n",
      "34    353  51703      593  1237651213360627946\n",
      "35    605  52353      174  1237651250430148697\n",
      "36    614  53437      146  1237651250972983503\n",
      "37   6320  56453       40  1237651250976784508\n",
      "38    605  52353      571  1237651251504283829\n",
      "39    437  51876      441  1237651252018151683\n",
      "40    443  51873      514  1237651252021493772\n",
      "41    770  52282      449  1237651252028964956\n",
      "42    606  52365      515  1237651252578287713\n",
      "43    494  51915       89  1237651271364247574\n",
      "44    488  51914      161  1237651272431829033\n",
      "45    354  51792      617  1237651312681746456\n",
      "46    442  51882      113  1237651494687146132\n",
      "47   6375  56323      696  1237651497367437556\n",
      "48    486  51910       59  1237651537630658687\n",
      "49   6982  56444      996  1237651539257393229\n"
     ]
    }
   ],
   "source": [
    "best_obj_ids = crossmatch['original_ext_source_id'].unique()\n",
    "batch = best_obj_ids[:50]  # Test with a very small batch\n",
    "batch_ids = ','.join(map(str, batch))\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT plate, mjd, fiberid, bestObjID\n",
    "FROM SpecObj\n",
    "WHERE bestObjID IN ({batch_ids})\n",
    "\"\"\"\n",
    "result = SDSS.query_sql(query)\n",
    "if result is not None:\n",
    "    spec_df = result.to_pandas()\n",
    "    print(spec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fbdc3a8-d1cc-49c9-b652-2383387945e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/2676.\n",
      "Processed batch 100/2676.\n",
      "Processed batch 200/2676.\n",
      "Processed batch 300/2676.\n",
      "Processed batch 400/2676.\n",
      "Processed batch 500/2676.\n",
      "Processed batch 600/2676.\n",
      "Processed batch 700/2676.\n",
      "Processed batch 800/2676.\n",
      "Processed batch 900/2676.\n",
      "Processed batch 1000/2676.\n",
      "Processed batch 1100/2676.\n",
      "Processed batch 1200/2676.\n",
      "Processed batch 1300/2676.\n",
      "Processed batch 1400/2676.\n",
      "Processed batch 1500/2676.\n",
      "Processed batch 1600/2676.\n",
      "Processed batch 1700/2676.\n",
      "Processed batch 1800/2676.\n",
      "Processed batch 1900/2676.\n",
      "Processed batch 2000/2676.\n",
      "Processed batch 2100/2676.\n",
      "Processed batch 2200/2676.\n",
      "Processed batch 2300/2676.\n",
      "Processed batch 2400/2676.\n",
      "Processed batch 2500/2676.\n",
      "Processed batch 2600/2676.\n",
      "Query completed. Results saved to 'specobj_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store results\n",
    "spec_df = pd.DataFrame()\n",
    "\n",
    "# Define a small batch size for querying\n",
    "batch_size = 50  # Small batch size to avoid exceeding query limits\n",
    "max_retries = 3  # Number of retries for failed batches\n",
    "\n",
    "# Process bestObjIDs in small batches\n",
    "for batch_index, i in enumerate(range(0, len(best_obj_ids), batch_size), start=1):\n",
    "    batch = best_obj_ids[i:i + batch_size]\n",
    "    batch_ids = ','.join(map(str, batch))  # Properly format IDs\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT plate, mjd, fiberid, bestObjID\n",
    "    FROM SpecObj\n",
    "    WHERE bestObjID IN ({batch_ids})\n",
    "    \"\"\"\n",
    "    \n",
    "    retries = 0\n",
    "    success = False\n",
    "    \n",
    "    while retries < max_retries and not success:\n",
    "        try:\n",
    "            # Query the SDSS database\n",
    "            result = SDSS.query_sql(query)\n",
    "            if result is not None:\n",
    "                # Append results to the DataFrame\n",
    "                spec_df = pd.concat([spec_df, result.to_pandas()], ignore_index=True)\n",
    "            \n",
    "            # Print progress every 100 batches\n",
    "            if batch_index % 100 == 0 or batch_index == 1:\n",
    "                print(f\"Processed batch {batch_index}/{len(best_obj_ids) // batch_size + 1}.\")\n",
    "            \n",
    "            success = True  # Mark this batch as successful\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error processing batch {i} to {i + len(batch)}: {e}. Retrying ({retries}/{max_retries})...\")\n",
    "            time.sleep(2)  # Add a short delay before retrying\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Batch {i} to {i + len(batch)} failed after {max_retries} retries.\")\n",
    "\n",
    "# Save the results to a file\n",
    "spec_df.to_csv(\"specobj_results.csv\", index=False)\n",
    "print(\"Query completed. Results saved to 'specobj_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "626d3042-0428-4862-bdec-044f948795c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>bestObjID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303</td>\n",
       "      <td>51615</td>\n",
       "      <td>205</td>\n",
       "      <td>1237648703516442877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>299</td>\n",
       "      <td>51671</td>\n",
       "      <td>179</td>\n",
       "      <td>1237648704049840359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>51943</td>\n",
       "      <td>309</td>\n",
       "      <td>1237648704586973290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311</td>\n",
       "      <td>51665</td>\n",
       "      <td>443</td>\n",
       "      <td>1237648705670152326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>364</td>\n",
       "      <td>52000</td>\n",
       "      <td>380</td>\n",
       "      <td>1237648705678213667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133789</th>\n",
       "      <td>7572</td>\n",
       "      <td>56944</td>\n",
       "      <td>710</td>\n",
       "      <td>1237680273108960003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133790</th>\n",
       "      <td>6265</td>\n",
       "      <td>56248</td>\n",
       "      <td>512</td>\n",
       "      <td>1237680285996155390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133791</th>\n",
       "      <td>6295</td>\n",
       "      <td>56536</td>\n",
       "      <td>434</td>\n",
       "      <td>1237680303715582376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133792</th>\n",
       "      <td>6515</td>\n",
       "      <td>56536</td>\n",
       "      <td>932</td>\n",
       "      <td>1237680308016841309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133793</th>\n",
       "      <td>5960</td>\n",
       "      <td>56097</td>\n",
       "      <td>436</td>\n",
       "      <td>1237680327333381385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133794 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        plate    mjd  fiberid            bestObjID\n",
       "0         303  51615      205  1237648703516442877\n",
       "1         299  51671      179  1237648704049840359\n",
       "2         300  51943      309  1237648704586973290\n",
       "3         311  51665      443  1237648705670152326\n",
       "4         364  52000      380  1237648705678213667\n",
       "...       ...    ...      ...                  ...\n",
       "133789   7572  56944      710  1237680273108960003\n",
       "133790   6265  56248      512  1237680285996155390\n",
       "133791   6295  56536      434  1237680303715582376\n",
       "133792   6515  56536      932  1237680308016841309\n",
       "133793   5960  56097      436  1237680327333381385\n",
       "\n",
       "[133794 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specobj = pd.read_csv('/Users/tharacaba/Downloads/ProjectBig/specobj_results.csv')\n",
    "specobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ec8561-7cb4-4e06-b50a-e4968a791b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 77 duplicate entries in 'original_ext_source_id'.\n",
      "Duplicate entries:\n",
      "                  SOURCE_ID  original_ext_source_id\n",
      "91       921068247868697728     1237653587407536295\n",
      "92       921068243577055232     1237653587407536295\n",
      "349      706252331821810944     1237660961862189211\n",
      "350      706252331823294464     1237660961862189211\n",
      "406     1541291590181192320     1237661852007727118\n",
      "...                     ...                     ...\n",
      "131800   147929260770079872     1237660558135656804\n",
      "132095   148055807685252352     1237660559209332868\n",
      "132096   148055807686085120     1237660559209332868\n",
      "132667  3706417688227261568     1237661970652397647\n",
      "132668  3706417692523377408     1237661970652397647\n",
      "\n",
      "[77 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in original_ext_source_id\n",
    "duplicate_ids = crossmatch[crossmatch.duplicated(subset='original_ext_source_id', keep=False)]\n",
    "\n",
    "if duplicate_ids.empty:\n",
    "    print(\"All IDs in 'original_ext_source_id' are unique.\")\n",
    "else:\n",
    "    print(f\"Found {len(duplicate_ids)} duplicate entries in 'original_ext_source_id'.\")\n",
    "    print(\"Duplicate entries:\")\n",
    "    print(duplicate_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f99396d-1ac5-46a0-b55f-246366d64fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (0 to 1000 entries)...\n",
      "Finished processing batch 1.\n",
      "Processing batch 2 (1000 to 2000 entries)...\n",
      "Finished processing batch 2.\n",
      "Processing batch 3 (2000 to 3000 entries)...\n",
      "Finished processing batch 3.\n",
      "Processing batch 4 (3000 to 4000 entries)...\n",
      "Finished processing batch 4.\n",
      "Processing batch 5 (4000 to 5000 entries)...\n",
      "Finished processing batch 5.\n",
      "Processing batch 6 (5000 to 6000 entries)...\n",
      "Finished processing batch 6.\n",
      "Processing batch 7 (6000 to 7000 entries)...\n",
      "Finished processing batch 7.\n",
      "Processing batch 8 (7000 to 8000 entries)...\n",
      "Finished processing batch 8.\n",
      "Processing batch 9 (8000 to 9000 entries)...\n",
      "Finished processing batch 9.\n",
      "Processing batch 10 (9000 to 10000 entries)...\n",
      "Finished processing batch 10.\n",
      "Processing batch 11 (10000 to 11000 entries)...\n",
      "Finished processing batch 11.\n",
      "Processing batch 12 (11000 to 12000 entries)...\n",
      "Finished processing batch 12.\n",
      "Processing batch 13 (12000 to 13000 entries)...\n",
      "Finished processing batch 13.\n",
      "Processing batch 14 (13000 to 14000 entries)...\n",
      "Finished processing batch 14.\n",
      "Processing batch 15 (14000 to 15000 entries)...\n",
      "Finished processing batch 15.\n",
      "Processing batch 16 (15000 to 16000 entries)...\n",
      "Finished processing batch 16.\n",
      "Processing batch 17 (16000 to 17000 entries)...\n",
      "Finished processing batch 17.\n",
      "Processing batch 18 (17000 to 18000 entries)...\n",
      "Finished processing batch 18.\n",
      "Processing batch 19 (18000 to 19000 entries)...\n",
      "Finished processing batch 19.\n",
      "Processing batch 20 (19000 to 20000 entries)...\n",
      "Finished processing batch 20.\n",
      "Processing batch 21 (20000 to 21000 entries)...\n",
      "Finished processing batch 21.\n",
      "Processing batch 22 (21000 to 22000 entries)...\n",
      "Finished processing batch 22.\n",
      "Processing batch 23 (22000 to 23000 entries)...\n",
      "Finished processing batch 23.\n",
      "Processing batch 24 (23000 to 24000 entries)...\n",
      "Finished processing batch 24.\n",
      "Processing batch 25 (24000 to 25000 entries)...\n",
      "Finished processing batch 25.\n",
      "Processing batch 26 (25000 to 26000 entries)...\n",
      "Finished processing batch 26.\n",
      "Processing batch 27 (26000 to 27000 entries)...\n",
      "Finished processing batch 27.\n",
      "Processing batch 28 (27000 to 28000 entries)...\n",
      "Finished processing batch 28.\n",
      "Processing batch 29 (28000 to 29000 entries)...\n",
      "Finished processing batch 29.\n",
      "Processing batch 30 (29000 to 30000 entries)...\n",
      "Finished processing batch 30.\n",
      "Processing batch 31 (30000 to 31000 entries)...\n",
      "Finished processing batch 31.\n",
      "Processing batch 32 (31000 to 32000 entries)...\n",
      "Finished processing batch 32.\n",
      "Processing batch 33 (32000 to 33000 entries)...\n",
      "Finished processing batch 33.\n",
      "Processing batch 34 (33000 to 34000 entries)...\n",
      "Finished processing batch 34.\n",
      "Processing batch 35 (34000 to 35000 entries)...\n",
      "Finished processing batch 35.\n",
      "Processing batch 36 (35000 to 36000 entries)...\n",
      "Finished processing batch 36.\n",
      "Processing batch 37 (36000 to 37000 entries)...\n",
      "Finished processing batch 37.\n",
      "Processing batch 38 (37000 to 38000 entries)...\n",
      "Finished processing batch 38.\n",
      "Processing batch 39 (38000 to 39000 entries)...\n",
      "Finished processing batch 39.\n",
      "Processing batch 40 (39000 to 40000 entries)...\n",
      "Finished processing batch 40.\n",
      "Processing batch 41 (40000 to 41000 entries)...\n",
      "Finished processing batch 41.\n",
      "Processing batch 42 (41000 to 42000 entries)...\n",
      "Finished processing batch 42.\n",
      "Processing batch 43 (42000 to 43000 entries)...\n",
      "Finished processing batch 43.\n",
      "Processing batch 44 (43000 to 44000 entries)...\n",
      "Finished processing batch 44.\n",
      "Processing batch 45 (44000 to 45000 entries)...\n",
      "Finished processing batch 45.\n",
      "Processing batch 46 (45000 to 46000 entries)...\n",
      "Finished processing batch 46.\n",
      "Processing batch 47 (46000 to 47000 entries)...\n",
      "Finished processing batch 47.\n",
      "Processing batch 48 (47000 to 48000 entries)...\n",
      "Finished processing batch 48.\n",
      "Processing batch 49 (48000 to 49000 entries)...\n",
      "Finished processing batch 49.\n",
      "Processing batch 50 (49000 to 50000 entries)...\n",
      "Finished processing batch 50.\n",
      "Processing batch 51 (50000 to 51000 entries)...\n",
      "Finished processing batch 51.\n",
      "Processing batch 52 (51000 to 52000 entries)...\n",
      "Finished processing batch 52.\n",
      "Processing batch 53 (52000 to 53000 entries)...\n",
      "Finished processing batch 53.\n",
      "Processing batch 54 (53000 to 54000 entries)...\n",
      "Finished processing batch 54.\n",
      "Processing batch 55 (54000 to 55000 entries)...\n",
      "Finished processing batch 55.\n",
      "Processing batch 56 (55000 to 56000 entries)...\n",
      "Finished processing batch 56.\n",
      "Processing batch 57 (56000 to 57000 entries)...\n",
      "Finished processing batch 57.\n",
      "Processing batch 58 (57000 to 58000 entries)...\n",
      "Finished processing batch 58.\n",
      "Processing batch 59 (58000 to 59000 entries)...\n",
      "Finished processing batch 59.\n",
      "Processing batch 60 (59000 to 60000 entries)...\n",
      "Finished processing batch 60.\n",
      "Processing batch 61 (60000 to 61000 entries)...\n",
      "Finished processing batch 61.\n",
      "Processing batch 62 (61000 to 62000 entries)...\n",
      "Finished processing batch 62.\n",
      "Processing batch 63 (62000 to 63000 entries)...\n",
      "Finished processing batch 63.\n",
      "Processing batch 64 (63000 to 64000 entries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The following header keyword is invalid or follows an unrecognized non-standard convention:\n",
      "MC1TEMDN=-0.00000000000000E+00 / sp1 mech median temp                            [astropy.io.fits.card]\n",
      "WARNING: The following header keyword is invalid or follows an unrecognized non-standard convention:\n",
      "MC1TBCT =-0.00000000000000E+00 / sp1 mech Temp_Blue_Cam_Top                      [astropy.io.fits.card]\n",
      "WARNING: The following header keyword is invalid or follows an unrecognized non-standard convention:\n",
      "MC1TEMDN=-0.00000000000000E+00 / sp1 mech median temp                            [astropy.io.fits.card]\n",
      "WARNING: The following header keyword is invalid or follows an unrecognized non-standard convention:\n",
      "MC1TBCT =-0.00000000000000E+00 / sp1 mech Temp_Blue_Cam_Top                      [astropy.io.fits.card]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Error downloading spectrum: plate=9353, mjd=57814, fiberid=592. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=4261, mjd=55503, fiberid=656. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Finished processing batch 64.\n",
      "Processing batch 65 (64000 to 65000 entries)...\n",
      "  Error downloading spectrum: plate=5329, mjd=55946, fiberid=270. Error: HTTPSConnectionPool(host='skyserver.sdss.org', port=443): Read timed out. (read timeout=60). Retrying...\n",
      "  Error downloading spectrum: plate=471, mjd=51924, fiberid=386. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=5732, mjd=56326, fiberid=820. Error: HTTPSConnectionPool(host='skyserver.sdss.org', port=443): Read timed out. (read timeout=60). Retrying...\n",
      "  Error downloading spectrum: plate=8529, mjd=58013, fiberid=620. Error: HTTPSConnectionPool(host='skyserver.sdss.org', port=443): Read timed out. (read timeout=60). Retrying...\n",
      "  Error downloading spectrum: plate=9359, mjd=58056, fiberid=970. Error: HTTPSConnectionPool(host='skyserver.sdss.org', port=443): Read timed out. (read timeout=60). Retrying...\n",
      "  Error downloading spectrum: plate=4455, mjd=55539, fiberid=988. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "Finished processing batch 65.\n",
      "Processing batch 66 (65000 to 66000 entries)...\n",
      "  Error downloading spectrum: plate=5778, mjd=56328, fiberid=774. Error: <urlopen error [Errno 54] Connection reset by peer>. Retrying...\n",
      "  Error downloading spectrum: plate=6474, mjd=56362, fiberid=564. Error: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')). Retrying...\n",
      "  Error downloading spectrum: plate=6436, mjd=56363, fiberid=42. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=6417, mjd=56308, fiberid=914. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=4496, mjd=55544, fiberid=288. Error: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')). Retrying...\n",
      "  Error downloading spectrum: plate=2498, mjd=54169, fiberid=144. Error: The read operation timed out. Retrying...\n",
      "  Error downloading spectrum: plate=7892, mjd=57333, fiberid=196. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=7047, mjd=56572, fiberid=704. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=6152, mjd=56164, fiberid=876. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=11045, mjd=58485, fiberid=23. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=1154, mjd=53083, fiberid=593. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=8532, mjd=58022, fiberid=871. Error: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')). Retrying...\n",
      "Finished processing batch 66.\n",
      "Processing batch 67 (66000 to 67000 entries)...\n",
      "  Error downloading spectrum: plate=7303, mjd=57013, fiberid=437. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n",
      "  Error downloading spectrum: plate=7284, mjd=56683, fiberid=525. Error: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying...\n"
     ]
    }
   ],
   "source": [
    "# Load the spectra information\n",
    "spectra_info = specobj  # Replace with your actual DataFrame\n",
    "\n",
    "# Define the output directory for spectra\n",
    "output_dir = \"/Users/tharacaba/Downloads/ProjectBig/Spectra\"\n",
    "parquet_dir = \"/Users/tharacaba/Downloads/ProjectBig/Spectra_Parquet\"  # Directory for Parquet files\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Define batch size and retry settings\n",
    "batch_size = 1000  # Number of spectra to process in one batch\n",
    "max_retries = 3  # Number of retries for failed downloads\n",
    "\n",
    "# Initialize a list to log failed spectra\n",
    "failed_spectra = []\n",
    "\n",
    "# Process spectra in batches\n",
    "for batch_start in range(0, len(spectra_info), batch_size):\n",
    "    # Get the current batch\n",
    "    batch = spectra_info.iloc[batch_start:batch_start + batch_size]\n",
    "    print(f\"Processing batch {batch_start // batch_size + 1} ({batch_start} to {batch_start + len(batch)} entries)...\")\n",
    "    \n",
    "    for idx, row in batch.iterrows():\n",
    "        plate, mjd, fiberid = row['plate'], row['mjd'], row['fiberid']\n",
    "        filename = os.path.join(output_dir, f\"spectrum_plate{plate}_mjd{mjd}_fiber{fiberid}.fits\")\n",
    "        \n",
    "        # Define Parquet filename\n",
    "        parquet_file = os.path.join(parquet_dir, f\"spectrum_plate{plate}_mjd{mjd}_fiber{fiberid}.parquet\")\n",
    "        \n",
    "        # Skip download if Parquet file already exists\n",
    "        if os.path.exists(parquet_file):\n",
    "            continue\n",
    "        \n",
    "        # Retry mechanism\n",
    "        retries = 0\n",
    "        success = False\n",
    "        while retries < max_retries and not success:\n",
    "            try:\n",
    "                spectrum = SDSS.get_spectra(plate=plate, mjd=mjd, fiberID=fiberid)\n",
    "                if spectrum:\n",
    "                    # Save the spectrum to the output directory\n",
    "                    spectrum[0].writeto(filename, overwrite=True)\n",
    "                    success = True\n",
    "                else:\n",
    "                    print(f\"  Spectrum not found: plate={plate}, mjd={mjd}, fiberid={fiberid}\")\n",
    "                    retries = max_retries  # Exit retry loop as it's a permanent failure\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"  Error downloading spectrum: plate={plate}, mjd={mjd}, fiberid={fiberid}. Error: {e}. Retrying...\")\n",
    "                time.sleep(2)  # Short delay before retrying\n",
    "        \n",
    "        if not success:\n",
    "            failed_spectra.append({'plate': plate, 'mjd': mjd, 'fiberid': fiberid})\n",
    "            print(f\"  Failed to download spectrum after {max_retries} retries: plate={plate}, mjd={mjd}, fiberid={fiberid}\")\n",
    "    \n",
    "    print(f\"Finished processing batch {batch_start // batch_size + 1}.\")\n",
    "\n",
    "# Save the failed spectra log\n",
    "if failed_spectra:\n",
    "    failed_df = pd.DataFrame(failed_spectra)\n",
    "    failed_df.to_csv(\"failed_spectra.csv\", index=False)\n",
    "    print(f\"Failed spectra logged to 'failed_spectra.csv'.\")\n",
    "\n",
    "print(\"All spectra processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4e65cb-6482-44cb-8227-c11eb18fbcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing existing FITS files...\n",
      "Finished processing existing FITS files.\n"
     ]
    }
   ],
   "source": [
    "# Define directories\n",
    "fits_dir = \"/Users/tharacaba/Downloads/ProjectBig/Spectra\"  # Directory with FITS files\n",
    "parquet_dir = \"/Users/tharacaba/Downloads/ProjectBig/Spectra_Parquet\"  # Directory for Parquet files\n",
    "os.makedirs(parquet_dir, exist_ok=True)  # Ensure the Parquet directory exists\n",
    "\n",
    "# Load the specObj list\n",
    "spectra_info = specobj  # Replace with your file\n",
    "\n",
    "# Define batch size and retry settings\n",
    "batch_size = 1000  # Number of spectra to process in one batch\n",
    "max_retries = 3  # Number of retries for failed downloads\n",
    "\n",
    "# Process existing FITS files\n",
    "print(\"Processing existing FITS files...\")\n",
    "for fits_file in os.listdir(fits_dir):\n",
    "    if fits_file.endswith(\".fits\"):\n",
    "        # Extract plate, mjd, and fiberid from the filename\n",
    "        file_parts = fits_file.split(\"_\")\n",
    "        plate = file_parts[1][5:]\n",
    "        mjd = file_parts[2][3:]\n",
    "        fiberid = file_parts[3][5:].replace(\".fits\", \"\")\n",
    "        \n",
    "        # Define Parquet filename\n",
    "        parquet_file = os.path.join(parquet_dir, f\"spectrum_plate{plate}_mjd{mjd}_fiber{fiberid}.parquet\")\n",
    "        \n",
    "        # Skip if Parquet file already exists\n",
    "        if os.path.exists(parquet_file):\n",
    "            continue\n",
    "        \n",
    "        # Open FITS file and extract data\n",
    "        fits_path = os.path.join(fits_dir, fits_file)\n",
    "        try:\n",
    "            with fits.open(fits_path) as hdul:\n",
    "                data = Table(hdul[1].data)\n",
    "                \n",
    "                # Convert to native byte order\n",
    "                flux = np.array(data[\"flux\"]).byteswap().newbyteorder()\n",
    "                loglam = np.array(data[\"loglam\"]).byteswap().newbyteorder()\n",
    "            \n",
    "            # Save to Parquet\n",
    "            df = pd.DataFrame({\"loglam\": loglam, \"flux\": flux})\n",
    "            df.to_parquet(parquet_file, index=False)\n",
    "            \n",
    "            # Delete the FITS file\n",
    "            os.remove(fits_path)\n",
    "            #print(f\"Processed and deleted: {fits_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fits_file}: {e}\")\n",
    "print(\"Finished processing existing FITS files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "206ac4fe-c06d-4c6c-8a3e-552de27b2a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flux ratios calculated, saved to 'flux_ratios.csv', and Parquet files deleted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define directories\n",
    "parquet_dir = \"/Users/tharacaba/Downloads/ProjectBig/Spectra_Parquet\"  # Directory for Parquet files\n",
    "\n",
    "# Load the DataFrame with bestObjID, plate, mjd, and fiberid\n",
    "metadata_df = specobj  # Replace with your metadata file\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Process each Parquet file\n",
    "for parquet_file in os.listdir(parquet_dir):\n",
    "    if parquet_file.endswith(\".parquet\"):\n",
    "        # Extract plate, mjd, and fiberid from the filename\n",
    "        file_parts = parquet_file.split(\"_\")\n",
    "        plate = int(file_parts[1][5:])\n",
    "        mjd = int(file_parts[2][3:])\n",
    "        fiberid = int(file_parts[3][5:].replace(\".parquet\", \"\"))\n",
    "        \n",
    "        # Load the Parquet file\n",
    "        parquet_path = os.path.join(parquet_dir, parquet_file)\n",
    "        try:\n",
    "            data = pd.read_parquet(parquet_path)\n",
    "            \n",
    "            # Convert loglam to lam\n",
    "            data['lam'] = 10 ** data['loglam']\n",
    "            \n",
    "            # Define wavelength ranges\n",
    "            range1 = (3300, 6800)  # First range\n",
    "            range2 = (6400, 9150)  # Second range\n",
    "            \n",
    "            # Calculate integrated flux for each range\n",
    "            mask1 = (data['lam'] >= range1[0]) & (data['lam'] <= range1[1])\n",
    "            mask2 = (data['lam'] >= range2[0]) & (data['lam'] <= range2[1])\n",
    "            \n",
    "            flux1 = np.trapz(data.loc[mask1, 'flux'], data.loc[mask1, 'lam'])\n",
    "            flux2 = np.trapz(data.loc[mask2, 'flux'], data.loc[mask2, 'lam'])\n",
    "            \n",
    "            # Calculate the flux ratio\n",
    "            flux_ratio = flux1 / flux2 if flux2 != 0 else np.nan\n",
    "            \n",
    "            # Find the bestObjID from metadata\n",
    "            matched_row = metadata_df[\n",
    "                (metadata_df['plate'] == plate) & \n",
    "                (metadata_df['mjd'] == mjd) & \n",
    "                (metadata_df['fiberid'] == fiberid)\n",
    "            ]\n",
    "            \n",
    "            if not matched_row.empty:\n",
    "                bestObjID = matched_row['bestObjID'].values[0]\n",
    "                results.append({'bestObjID': bestObjID, 'plate': plate, 'mjd': mjd, 'fiberid': fiberid, 'flux_ratio': flux_ratio})\n",
    "            else:\n",
    "                print(f\"No match found for Parquet file: {parquet_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {parquet_file}: {e}\")\n",
    "        finally:\n",
    "            # Delete the Parquet file after processing\n",
    "            os.remove(parquet_path)\n",
    "            #print(f\"Deleted Parquet file: {parquet_file}\")\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"flux_ratios.csv\", index=False)\n",
    "\n",
    "print(\"Flux ratios calculated, saved to 'flux_ratios.csv', and Parquet files deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6589b34d-68e8-4241-a704-2be3aa799784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bestObjID</th>\n",
       "      <th>plate</th>\n",
       "      <th>mjd</th>\n",
       "      <th>fiberid</th>\n",
       "      <th>flux_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1237655693011845153</td>\n",
       "      <td>918</td>\n",
       "      <td>52404</td>\n",
       "      <td>515</td>\n",
       "      <td>1.028455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1237665024914423991</td>\n",
       "      <td>2121</td>\n",
       "      <td>54180</td>\n",
       "      <td>475</td>\n",
       "      <td>1.027186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237667429022629905</td>\n",
       "      <td>2295</td>\n",
       "      <td>53734</td>\n",
       "      <td>201</td>\n",
       "      <td>1.742462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1237668272448733204</td>\n",
       "      <td>2768</td>\n",
       "      <td>54265</td>\n",
       "      <td>520</td>\n",
       "      <td>0.898819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1237662236931457149</td>\n",
       "      <td>1233</td>\n",
       "      <td>52734</td>\n",
       "      <td>185</td>\n",
       "      <td>1.587512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>1237668294982565897</td>\n",
       "      <td>2490</td>\n",
       "      <td>54179</td>\n",
       "      <td>322</td>\n",
       "      <td>1.160761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>1237655498126852309</td>\n",
       "      <td>910</td>\n",
       "      <td>52377</td>\n",
       "      <td>189</td>\n",
       "      <td>1.303406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>1237651752391213235</td>\n",
       "      <td>275</td>\n",
       "      <td>51910</td>\n",
       "      <td>324</td>\n",
       "      <td>1.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>1237663917882802416</td>\n",
       "      <td>1876</td>\n",
       "      <td>54464</td>\n",
       "      <td>491</td>\n",
       "      <td>1.116426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739</th>\n",
       "      <td>1237655501889470786</td>\n",
       "      <td>978</td>\n",
       "      <td>52441</td>\n",
       "      <td>311</td>\n",
       "      <td>0.979367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2740 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                bestObjID  plate    mjd  fiberid  flux_ratio\n",
       "0     1237655693011845153    918  52404      515    1.028455\n",
       "1     1237665024914423991   2121  54180      475    1.027186\n",
       "2     1237667429022629905   2295  53734      201    1.742462\n",
       "3     1237668272448733204   2768  54265      520    0.898819\n",
       "4     1237662236931457149   1233  52734      185    1.587512\n",
       "...                   ...    ...    ...      ...         ...\n",
       "2735  1237668294982565897   2490  54179      322    1.160761\n",
       "2736  1237655498126852309    910  52377      189    1.303406\n",
       "2737  1237651752391213235    275  51910      324    1.141400\n",
       "2738  1237663917882802416   1876  54464      491    1.116426\n",
       "2739  1237655501889470786    978  52441      311    0.979367\n",
       "\n",
       "[2740 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
